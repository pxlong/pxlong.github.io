<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~cbfinn/ -->
<html>
    <head>
        <!-- meta http-equiv="Content-Type" content="text/html; charset=windows-1252" / -->
        <!-- meta name="viewport" content="width=device-width, initial-scale=1" / -->
        <meta name="google-site-verification" content="2TkgHhCGDji8Ye9iKyYwjIl3Er8imhZ2fC7vEhjHkPc" />
        <meta name="viewport" content="width=800" />
        <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org" />
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    heading2 {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 42px;
    }
    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="./pxlong_files/pxlong_icon.png">
  <title>Pinxin Long</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">

  <link href="./pxlong_files/css" rel="stylesheet" type="text/css">
  <style id="dark-reader-style" type="text/css">@media screen {

/* Leading rule */
html {
  -webkit-filter: brightness(110%) contrast(90%) grayscale(20%) sepia(10%) !important;
}

/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}

/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}

/* Page background */
html {
  background: rgb(255,255,255) !important;
}

}</style></head>

<body>
    <div id="StayFocusd-infobar" style="display: none; top: 2400px;">
        <img src="chrome-extension://laankejkbhbdhmipfmgcngdelahlfoji/common/img/eye_19x19_red.png">
        <span id="StayFocusd-infobar-msg"></span>
        <span id="StayFocusd-infobar-links">
            <a id="StayFocusd-infobar-never-show">hide forever</a>&nbsp;&nbsp;|&nbsp;&nbsp;
            <a id="StayFocusd-infobar-hide">hide once</a>
        </span>
    </div>

  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody>
          <tr>
              <td>
                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                      <tbody><tr>
                          <td width="100%" valign="middle">
                              <p align="center">
                                  <name>Pinxin Long</name><br>
                                  pinxinlong at gmail dot com
                              </p>
                              <p>
                                  I am a Staff Software Engineer at Baidu Apollo working on the Apollo Navigation Pilot (i.e. Baidu's Full Self-Driving (FSD), an advanced driver-assist system) 
                                  which has been successfully deployed on <a href="https://www.jiyue-auto.com/" target="_blank">Jiyue-01 cars</a> and delivered to customers by the end of 2023.
                                  Before delving into self-driving cars, I was a Researcher and Software Engineer under the mentorship of Prof.<a href="https://scholar.google.com/citations?user=yveq40QAAAAJ&hl=zh-CN" target="_blank"> Ruigang Yang</a>
                                  at Robotics and Auto-driving Lab (RAL), Baidu Research, focusing on the 
                                  design and development of the <a href="https://www.youtube.com/watch?v=KFcNf_k0E_M&t=4s&ab_channel=BaiduInc." targe="_blank"> Autonomous Excavator Systems</a>.
                                  Prior to this role, I held the position of Research Scientist at <a href="https://www.youtube.com/watch?v=HxTTcap1Wrk&ab_channel=TingxiangFan" target="_blank">Dorabot Inc.</a> and served as a Research Assistant 
                                  collaborating with <a href="https://sites.google.com/site/panjia/" target="_blank">Prof. Jia Pan</a> on projects related to multi-robot collision avoidance.
                                  
                                  
                                  
                              </p>
                              <!--
                              <p>
                                  Prior to CityU, I worked as a research assistant in <a href="http://vcc.szu.edu.cn/" target="_blank">Visual Computing Center (VCC)</a> at
                                  <a href="http://www.siat.ac.cn/" target="_blank">
                                  Shenzhen Institutes of Advanced Technology (SIAT)</a>, <a href="http://www.cas.cn/" target="_blank">Chinese Academy of Sciences (CAS)</a>, where I involved in
                                  several research projects, including two exciting robotic auto-scanning projects with the <a href="https://www.youtube.com/watch?v=NxOzdXR_aN0&t=4s" target="_blank">
                                      PR2 robot</a>.
                                  Before coming to Shenzhen, I received a Bachelor's degree in Automation at
                                  <a href="http://www.uestc.edu.cn/" target="_blank">UESTC</a>, you can find some videos about my undergraduate projects below. 
                              </p>
                              -->
                              <!--
                              <p>
                                  <strong>
                                      I am looking for a full time job in the <I>Autonomous Driving</I> industry.
                                      If you are interested in talking with me about any work opportunities,
                                      please feel free to <a href="mailto:pinxinlong@gmail.com">drop me a line</a>.
                                  </strong>
                              </p>
                              -->
                              <p align="center">
                                  <!-- <a href="./pxlong_files/pxlong_cv.pdf" target="_blank">CV</a> &nbsp;/&nbsp; -->
                                  <a href="https://scholar.google.com/citations?user=m3tbW2kAAAAJ&hl=en" target="_blank">Google Scholar</a> 
                                  <!-- <a href="http://www.github.com/pxlong/" target="_blank"> GitHub </a> -->
                              </p>
                          </td>
                          <!--
                          <td width="33%">
                               <img src="./pxlong_files/pxlong_small.png">
                               <p align="center">&copy;  Mavis Chen 2017 </p>
                          </td>
                          -->
                      </tr>
                      </tbody>
                  </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
              <tr>
                  <td>
                      <h2>News</h2>
                      <ul>
                          <li> 2021.06 - The Autonomous Excavator paper is published in <strong style="color:red">Science Robotics</strong>. 
                          <li> 2020.06 - Our paper on multi-robot collision avoidance is published in <strong style="color:red">International Journal of Robotics Research (IJRR)</strong>. 
                          <li> 2017.06 - I presented our work on exploring deep neural networks for multi-robot navigation in the
                              <a href="http://webdiis.unizar.es/~edumonti/17_ICRA_WS/cfp.html" target="_blank">
                              Multi-robot Perception-Driven Control and Planning Workshop</a> at ICRA 2017.</li>
                          <li> 2016.12 - I presented our work on multi-agent collision avoidance in the
                              <a href="https://sites.google.com/site/malicnips2016/" target="_blank">
                              Learning, Inference and Control of Multi-Agent Systems Workshop</a> at NIPS 2016.</li>
                      </ul>
                  </td>
              </tr>


        
        </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <h2>Research</h2>
          <p>
            I am interested in artificial intelligence, machine learning and robotics.
            Representative papers are <span class="highlight">highlighted</span> (* denotes equal contribution).
          </p>
        </td>
      </tr>
    </tbody></table>

     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody>

            <tr onmouseout="hybrid_stop()" onmouseover="hybrid_start()" bgcolor="#ffffd0">
                <td width="25%" valign="bottom" >
                    <div class="one">
                        <div class="two" id="hybrid_image" style="opacity: 0;"><img src="./pxlong_files/aes_move2dump.gif" width="150"></div>
                        <img src="./pxlong_files/aes_dig.gif" width="150">
                    </div>
                    <script type="text/javascript">
                     function hybrid_start() {
                         document.getElementById('hybrid_image').style.opacity = "1";
                     }
                     function hybrid_stop() {
                         document.getElementById('hybrid_image').style.opacity = "0";
                     }
                     hybrid_stop()
                    </script>
                </td>
                <td valign="top" width="75%">
                    <p>
                        <a href="https://www.science.org/doi/abs/10.1126/scirobotics.abc3164" target="_blank">
                            <papertitle>An autonomous excavator system for material loading tasks</papertitle></a><br>
                        <a href="https://www.cs.unc.edu/~zlj/" target="_blank">Liangjun Zhang</a>,
                            Jinxin Zhao, 
                        <strong>Pinxin Long</strong>,
                            Liyang Wang, Lingfeng Qian, Feixiang Lu, 
                        <a href="https://scholar.google.com/citations?user=2gudyEQAAAAJ&hl=zh-CN" target="_blank">Xibin Song</a>,

                            <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank">Dinesh Manocha</a> <br>
                        <em><strong>Science Robotics</strong> </em>, 2021. <br>
                        <a href="https://www.youtube.com/watch?v=7GGn0bkwjWg&ab_channel=GAMMAUMD" target="_blank">video</a>
                    </p><p></p>
                    <p>
                        We present an autonomous excavator system (AES) for material loading tasks. 
                        Our system can handle different environments and uses an architecture that combines perception and planning.
                        AES has been deployed for real-world operations for long periods and can operate robustly in challenging scenarios.
                    </p>
                </td>
            </tr>

            <tr onmouseout="hybrid_stop()" onmouseover="hybrid_start()">
                <td width="25%" valign="bottom" >
                    <div class="one">
                        <div class="two" id="hybrid_image" style="opacity: 0;"><img src="./pxlong_files/ur5_dig.png" width="150"></div>
                        <img src="./pxlong_files/ur5_dig.png" width="150">
                    </div>
                    <script type="text/javascript">
                     function hybrid_start() {
                         document.getElementById('hybrid_image').style.opacity = "1";
                     }
                     function hybrid_stop() {
                         document.getElementById('hybrid_image').style.opacity = "0";
                     }
                     hybrid_stop()
                    </script>
                </td>
                <td valign="top" width="75%">
                    <p>
                        <a href="https://arxiv.org/pdf/2010.14038.pdf" target="_blank">
                        <papertitle>Optimization-based framework for excavation trajectory generation</papertitle></a><br>
                        Yajue Yang, 
                        <strong>Pinxin Long</strong>,
                        <a href="https://scholar.google.com/citations?user=2gudyEQAAAAJ&hl=zh-CN" target="_blank">Xibin Song</a>,
                        <a href="https://sites.google.com/site/panjia/" target="_blank">Jia Pan</a>,
                        <a href="https://www.cs.unc.edu/~zlj/" target="_blank">Liangjun Zhang</a> <br>
                        <em>IEEE Robotics and Automation Letters (RAL)</em>, 2021 <br>
                        <a href="https://arxiv.org/abs/2010.14038" target="_blank">arXiv</a>
                    </p><p></p>
                    <p>
                        We present a novel optimization-based framework for autonomous excavator trajectory generation under various objectives, 
                        including minimum joint displacement and minimum time. 
                    </p>
                </td>
            </tr>

            <tr onmouseout="hybrid_stop()" onmouseover="hybrid_start()" bgcolor="#ffffd0">
                <td width="25%" valign="bottom" >
                    <div class="one">
                        <div class="two" id="hybrid_image" style="opacity: 0;"><img src="./pxlong_files/hybrid.gif"></div>
                        <img src="./pxlong_files/hybrid.png">
                    </div>
                    <script type="text/javascript">
                     function hybrid_start() {
                         document.getElementById('hybrid_image').style.opacity = "1";
                     }
                     function hybrid_stop() {
                         document.getElementById('hybrid_image').style.opacity = "0";
                     }
                     hybrid_stop()
                    </script>
                </td>
                <td valign="top" width="75%">
                    <p>
                        <a href="https://arxiv.org/pdf/1808.03841.pdf" target="_blank">
                            <papertitle>Distributed multi-robot collision avoidance via deep reinforcement learning for navigation in complex scenarios</papertitle></a><br>
                        Tingxiang Fan*,
                        <strong>Pinxin Long*</strong>,
                        Wenxi Liu,
                        <a href="https://sites.google.com/site/panjia/" target="_blank">Jia Pan</a> <br>
                        <em><strong>The International Journal of Robotics Research (IJRR)</strong> </em>, 2020. <br>
                        <a href="https://sites.google.com/view/hybridmrca/" target="_blank">project</a>
                        /
                        <a href="https://www.youtube.com/watch?v=HxTTcap1Wrk" target="_blank">video</a>
                        /
                        <a href="https://arxiv.org/abs/1808.03841/" target="_blank">arXiv</a>
                    </p><p></p>
                    <p>
                        We present a decentralized sensor-level collision avoidance policy for multi-robot systems.
                        The learned policy is also integrated into a hybrid control framework to further improve the policy's robustness and effectiveness.
                        Our learned policy enables a robot to make effective progress in a crowd without getting stuck. 
                    </p>
                </td>
            </tr>

            <tr onmouseout="hybrid_stop()" onmouseover="hybrid_start()">
                <td width="25%" valign="bottom" >
                    <div class="one">
                        <div class="two" id="hybrid_image" style="opacity: 0;"><img src="./pxlong_files/resilient1.png" width="150"></div>
                        <img src="./pxlong_files/resilient1.png" width="150">
                    </div>
                    <script type="text/javascript">
                     function hybrid_start() {
                         document.getElementById('hybrid_image').style.opacity = "1";
                     }
                     function hybrid_stop() {
                         document.getElementById('hybrid_image').style.opacity = "0";
                     }
                     hybrid_stop()
                    </script>
                </td>
                <td valign="top" width="75%">
                    <p>
                        <a href="https://arxiv.org/pdf/1910.09998.pdf" target="_blank">
                            <papertitle>Learning resilient behaviors for navigation under uncertainty</papertitle></a><br>
                            Tingxiang Fan, 
                            <strong>Pinxin Long</strong>,
                            <a href="https://scholar.google.com/citations?user=Y310sT0AAAAJ&hl=en" target="_blank">Wenxi Liu</a>,
                            <a href="https://sites.google.com/site/panjia/" target="_blank">Jia Pan</a>,
                            <a href="https://scholar.google.com/citations?user=yveq40QAAAAJ&hl=en" target="_blank">Ruigang Yang</a>,
                            <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank">Dinesh Manocha</a> <br>
                            
                        <em>International Conference on Robotics and Automation (ICRA) </em>, 2020. <br>
                        <a href="https://www.youtube.com/watch?v=KxRJp_Aanpo&ab_channel=TingxiangFan" target="_blank">video</a>
                        /
                        <a href="https://arxiv.org/abs/1910.09998/" target="_blank">arXiv</a>
                    </p><p></p>
                    <p>
                        We present a novel approach for uncertainty-aware navigation by introducing an uncertaintyaware predictor to model the environmental uncertainty, and 
                        propose a novel uncertainty-aware navigation network to learn resilient behaviors in the prior unknown environments
                    </p>
                </td>
            </tr>

            <tr onmouseout="rlslam_stop()" onmouseover="rlslam_start()">
                 <td width="25%" valign="bottom" >
                     <!-- h2>Publications</h2-->
                     <br><br>
                     <div class="one">
                         <div class="two" id="rlslam_image" style="opacity: 0;"><img src="./pxlong_files/rlslam.gif"></div>
                         <img src="./pxlong_files/rlslam.png">
                     </div>
                     <script async="" src="./pxlong_files/analytics.js">
                     </script>
                     <script type="text/javascript">
                      function rlslam_start() {
                          document.getElementById('rlslam_image').style.opacity = "1";
                      }
                      function rlslam_stop() {
                          document.getElementById('rlslam_image').style.opacity = "0";
                      }
                      rlslam_stop()
                     </script>
                 </td>

                 <td valign="top" width="75%">
                     <heading2><i></i></heading2><br>
                     <p>
                         <a href="https://arxiv.org/pdf/1810.00352.pdf" target="_blank">
                             <papertitle>Getting Robots Unfrozen and Unlost in Dense Pedestrian Crowds</papertitle></a><br>
                         Tingxiang Fan*, 
                         Xinjing Chen*, 
                         <a href="https://sites.google.com/site/panjia/" target="_blank">Jia Pan</a>, 
                         <strong>Pinxin Long</strong>,
                         Wenxi Liu,
                         <a href="https://scholar.google.com/citations?user=yveq40QAAAAJ&hl=zh-CN" target="_blank">Ruigang Yang</a>,
                         <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank">Dinesh Manocha</a> <br>
                         <em>IEEE Robotics and Automation Letters (RAL)</em>, 2019. <br>
                         <a href="https://sites.google.com/view/rlslam/" target="_blank">project</a>
                         /
                         <a href="https://www.youtube.com/watch?v=mx9eYeMGhVs" target="_blank">video</a>
                         /
                         <a href="https://arxiv.org/abs/1810.00352/" target="_blank">arXiv</a>
                     </p><p></p>
                     <p>
                            We aim to enable a mobile robot to navigate through environments with dense crowds, 
                            e.g., shopping malls, canteens, train stations, or airport terminals. 
                            Here we propose a navigation framework that handles the robot freezing and the navigation lost
                            problems simultaneously.
                     </p>
                 </td>
             </tr>

             
             <tr onmouseout="drl_stop()" onmouseover="drl_start()" bgcolor="#ffffd0">
                 <td width="25%" valign="bottom" >
                     <div class="one">
                         <div class="two" id="drl_image" style="opacity: 0;"><img src="./pxlong_files/drlmrca.gif"></div>
                         <img src="./pxlong_files/drlmrca.png">
                     </div>
                     <script type="text/javascript">
                      function drl_start() {
                          document.getElementById('drl_image').style.opacity = "1";
                      }
                      function drl_stop() {
                          document.getElementById('drl_image').style.opacity = "0";
                      }
                      drl_stop()
                     </script>
                 </td>
                 <td valign="top" width="75%">
                     <p>
                         <a href="https://arxiv.org/pdf/1709.10082.pdf" target="_blank">
                             <papertitle>Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning</papertitle></a><br>
                         <strong>Pinxin Long*</strong>,
                         Tingxiang Fan*,
                         Xinyi Liao,
                         Wenxi Liu,
                         <a href="http://www.dorabot.com/" target="_blank">Hao Zhang</a>,
                         <a href="https://sites.google.com/site/panjia/" target="_blank">Jia Pan</a> <br>
                         <em>International Conference on Robotics and Automation (ICRA)</em>, 2018. <br>
                         <a href="https://sites.google.com/view/drlmaca/" target="_blank">project</a>
                         /
                         <a href="https://www.youtube.com/watch?v=Uj1yAmlL5lk" target="_blank">video (youtube)</a>, 
                         <a href="https://www.bilibili.com/video/av14891012/" target="_blank">video (bilibili)</a>
                         /
                         <a href="https://arxiv.org/abs/1709.10082" target="_blank">arXiv</a>
                     </p><p></p>
                     <p>
                         As a first step toward reducing the performance gap between decentralized and centralized multi-robot collsion avoidance,
                         we present a multi-scenario multi-stage training framework to
                         find an optimal policy which is trained over a large number
                         of robots on rich, complex environments simultaneously using
                         a policy gradient based reinforcement algorithm.
                     </p>
                 </td>
             </tr>


             <tr onmouseout="deepmaca_stop()" onmouseover="deepmaca_start()" bgcolor="#ffffd0">
                 <td width="25%">
                     <heading2><i></i></heading2><br>
                     <div class="one">
                         <div class="two" id="deepmaca_image" style="opacity: 0;"><img src="./pxlong_files/deepmaca.gif" width="150" height="134"></div>
                         <img src="./pxlong_files/deepmaca.png" width="150" height="134">
                     </div>
                     <script type="text/javascript">
                      function deepmaca_start() {
                          document.getElementById('deepmaca_image').style.opacity = "1";
                      }
                      function deepmaca_stop() {
                          document.getElementById('deepmaca_image').style.opacity = "0";
                      }
                      deepmaca_stop()
                     </script>
                 </td>
                 <td valign="top" width="75%">
                     <p><a href="https://arxiv.org/pdf/1609.06838.pdf" target="_blank">
                         <papertitle>Deep-Learned Collision Avoidance Policy for Distributed Multi-Agent Navigation</papertitle></a><br>
                         <strong>Pinxin Long</strong>,  Wenxi Liu,
                         <a href="https://sites.google.com/site/panjia/" target="_blank">Jia Pan</a> <br>
                         <em>IEEE Robotics and Automation Letters (RAL)</em>, 2017 <br>
                         <a href="https://sites.google.com/view/deepmaca/" target="_blank">project (video)</a>
                         /
                         <a href="https://arxiv.org/abs/1609.06838" target="_blank">arXiv</a>
                     </p><p></p>
                     <p>
                         This paper is our first step toward learning a reactive
                         collision avoidance policy for multi-agent collision avoidance. By carefully designing the data collection process
                         and leveraging an end-to-end learning framework, our method
                         can learn a deep neural network based collision avoidance
                         policy which demonstrates an advantage over the state-of-theart ORCA policy in terms of ease of use,
                         success rate, and navigation performance.
                     </p>
                 </td>
             </tr>


             <tr onmouseout="dorapicker_stop()" onmouseover="dorapicker_start()">
                 <td width="25%">
                     <div class="one">
                         <div class="two" id="dorapicker_image" style="opacity: 0;"><img src="./pxlong_files/dorapicker.gif"></div>
                         <img src="./pxlong_files/dorapicker.png">
                     </div>
                     <script type="text/javascript">
                      function dorapicker_start() {
                          document.getElementById('dorapicker_image').style.opacity = "1";
                      }
                      function dorapicker_stop() {
                          document.getElementById('dorapicker_image').style.opacity = "0";
                      }
                      dorapicker_stop()
                     </script>
                     
                 </td>
                 <td valign="top" width="75%">
                     <p><a href="https://arxiv.org/pdf/1603.06317.pdf" target="_blank">
                         <papertitle>DoraPicker: An Autonomous Picking System for General Objects</papertitle></a><br>
                         <a href="http://www.dorabot.com/" target="_blank">Hao Zhang</a>,
                         <strong>Pinxin Long</strong>, Dandan Zhou, Zhongfeng Qian, Zheng Wang,
                         <a href="https://sites.google.com/site/weiweilab/" target="_blank">Weiwei Wan</a>,
                         <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank">Dinesh Manocha</a>, Chonhyon Park, Tommy Hu, Chao Cao, Yibo Chen, Marco Chow,
                         <a href="https://sites.google.com/site/panjia/" target="_blank">Jia Pan</a> <br>
                         <em>International Conference on Automation Science and Engineering (CASE)</em>, 2016 <br>
                         <!--  <strong style="color:green">Best Cognitive Robotics Paper Finalist</strong><br>-->
                         <a href="https://www.youtube.com/watch?v=E-rI2hBMRpk&feature=youtu.be" target="_blank">video</a>
                         /
                         <a href="http://arxiv.org/abs/1603.06317" target="_blank">arXiv</a>
                     </p><p></p>
                     <p>
                         We present our pick-and-place system in detail while highlighting
                         our design principles for the warehouse settings, including
                         the perception method that leverages knowledge about its
                         workspace, three grippers designed to handle a large variety of
                         different objects in terms of shape, weight and material, and
                         grasp planning in cluttered scenarios.
                     </p>
                 </td>
             </tr>
             

             <tr onmouseout="scene_stop()" onmouseover="scene_start()">
                 <td width="25%">
                     <div class="one">
                         <div class="two" id="scene_image" style="opacity: 0;"><img src="./pxlong_files/scene2.png"></div>
                         <img src="./pxlong_files/scene1.png">
                     </div>
                     <script type="text/javascript">
                      function scene_start() {
                          document.getElementById('scene_image').style.opacity = "1";
                      }
                      function scene_stop() {
                          document.getElementById('scene_image').style.opacity = "0";
                      }
                      scene_stop()
                     </script>
                 </td>
                 <td width="75%" valign="top">
                     <p><a href="http://kevinkaixu.net/papers/shi_cag15_scene.pdf" target="_blank" id="scene">
                         <papertitle>Data-Driven Contextual Modeling for 3D Scene Understanding</papertitle></a><br>
              <a href="http://www.yifeishi.net/" target="_blank">Yifei Shi</a>, <strong>Pinxin Long</strong>,
              <a href="http://kevinkaixu.net/" target="_blank">Kai Xu</a>, <a href="http://vcc.szu.edu.cn/~huihuang/" target="_blank">Hui Huang</a>, Yueshan Xiong <br>
              <em>Computer & Graphics (C&G)</em>, 2016<br>
              <a href="http://vcc.szu.edu.cn/research/2016/SceneModeling/" target="_blank">project</a>
                     </p>
                     <p>
                         We propose a data-driven approach to modeling contextual information
                         covering both intra-object part relations and inter-object object layouts.
                         Our method combines the detection of individual objects and object groups within the same framework,
                         enabling contextual analysis without knowing the objects in the scene a priori.
                     </p>
                 </td>
             </tr>
             
             <tr onmouseout="plant_stop()" onmouseover="plant_start()">
                 <td width="25%">
                     <div class="one">
                         <div class="two" id="plant_image" style="opacity: 0;"><img src="./pxlong_files/plant2.png"></div>
                    <img src="./pxlong_files/plant1.png">
                     </div>
                     <script type="text/javascript">
                      function plant_start() {
                     document.getElementById('plant_image').style.opacity = "1";
                      }
                      function plant_stop() {
                          document.getElementById('plant_image').style.opacity = "0";
                      }
                      plant_stop()
                     </script>
                 </td>
                 <td width="75%" valign="top">
                     <p><a href="http://kangxue.org/papers/plantCut.pdf" id="plant" target="_blank">
                         <papertitle>Full 3D Plant Reconstruction via Intrusive Acquisition</papertitle></a><br>
                         <a href="http://kangxue.org" target="_blank">Kangxue Yin</a>, <a href="http://vcc.szu.edu.cn/~huihuang/" target="_blank">Hui Huang</a>, 
                         <strong>Pinxin Long</strong>, Alexei Gaissinski, 
                         <a href="http://www.cs.mun.ca/~gong/" target="_blank">Minglun Gong</a>, 
                         <a href="https://www.cs.bgu.ac.il/~asharf/" target="_blank">Andrei Sharf</a>
                         <em>Computer Graphics Forum (CGF)</em>, 2016<br>
                         <a href="http://vcc.szu.edu.cn/research/2016/PlantCut/" target="_blank">project</a>
                         /
                         <a href="https://drive.google.com/open?id=0B1yWc4B_DEqPbnpWUGdoWGNwSVk" target="_blank">data</a>
                     </p>
                     <p>
                         We present an intrusive acquisition approach for acquiring and modeling of plants and foliage,
                         which disassembles the plant into disjoint parts that can be accurately scanned and reconstructed offline.
                     </p>
                 </td>
             </tr>
             
             <tr onmouseout="pr2scene_stop()" onmouseover="pr2scene_start()">
                 <td width="25%">
                     <div class="one">
                         <div class="two" id="pr2scene_image" style="opacity: 0;"><img src="./pxlong_files/pr2scene.gif"></div>
                    <img src="./pxlong_files/pr2scene.png">
                     </div>
                     <script type="text/javascript">
                      function pr2scene_start() {
                     document.getElementById('pr2scene_image').style.opacity = "1";
                      }
                      function pr2scene_stop() {
                          document.getElementById('pr2scene_image').style.opacity = "0";
                      }
                      pr2scene_stop()
                     </script>
                 </td>
                 <td valign="top" width="75%">
                     <p><a href="http://kevinkaixu.net/papers/xu_siga15_pr2scene.pdf" target="_blank">
                         <papertitle>Autoscanning for Coupled Scene Reconstruction and Proactive Object Analysis</papertitle></a><br>
                         <a href="http://kevinkaixu.net/" target="_blank">Kai Xu</a>, <a href="http://vcc.szu.edu.cn/~huihuang/" target="_blank">Hui Huang</a>,
                         <a href="http://www.yifeishi.net/" target="_blank">Yifei Shi</a>, <a href="http://haoli-china.github.io/Personal-Homepage/" target="_blank">Hao Li</a>,
                         <strong>Pinxin Long</strong>, Jianong Caichen,
                         Wei Sun, <a href="http://www.cs.sdu.edu.cn/~baoquan/" target="_blank">Baoquan Chen</a> <br>
                         <em>ACM Transactions on Graphics (SIGGRAPH Asia 2015)</em>, 2015 <br>
                         <!--  <strong style="color:green">Best Cognitive Robotics Paper Finalist</strong><br>-->
                         <a href="http://kevinkaixu.net/projects/pr2scene.html" target="_blank">project</a>
                         /
                         <a href="http://kevinkaixu.net/slides/xu_siga15_pr2scene.pdf" target="_blank">slides</a>
                         /
                         <a href="https://youtu.be/p26YuYVC0YI" target="_blank">video (youtube)</a>, 
                         <a href="http://v.youku.com/v_show/id_XMTMwNzYwNzQyMA==.html" target="_blank">video (youku)</a>
                         /
                         <a href="https://www.dropbox.com/s/wj3ezjjmlt6olj9/pr2scene_code.zip?dl=0" target="_blank">code</a>
                     </p><p></p>
                     <p>
                         We propose autonomous scene scanning by a robot to relieve humans from such a tedious task.
                         The presented algorithm interleaves between scene analysis for extracting objects and robot conducted validation for
                         improving the segmentation and object-aware reconstruction.
                     </p>
                 </td>
             </tr>
             
             <tr onmouseout="autoscan_stop()" onmouseover="autoscan_start()">
                 <td width="25%">
                     <div class="one">
                         <div class="two" id="autoscan_image" style="opacity: 0;"><img src="./pxlong_files/autoscan.gif"></div>
                         <img src="./pxlong_files/autoscan.png">
                     </div>
                     <script type="text/javascript">
                      function autoscan_start() {
                          document.getElementById('autoscan_image').style.opacity = "1";
                      }
                      function autoscan_stop() {
                          document.getElementById('autoscan_image').style.opacity = "0";
                      }
                      autoscan_stop()
                     </script>
                 </td>
                 <td valign="top" width="75%">
                     <p><a href="http://vcc.szu.edu.cn/commons/fileupload/upload.do?method=download&uploadId=2068">
                         <papertitle>Quality-driven Poisson-guided Autoscanning</papertitle></a><br>
                         <a href="http://shihaowu.net/" target="_blank">Shihao Wu</a>, Wei Sun, <strong>Pinxin Long</strong>, 
                         <a href="http://vcc.szu.edu.cn/~huihuang/" target="_blank">Hui Huang</a>, 
                         <a href="http://www.math.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>, 
                         <a href="http://www.cs.mun.ca/~gong/" target="_blank">Minglun Gong</a>, 
                         <a href="https://www.cgmi.uni-konstanz.de/" target="_blank">Oliver Deussen</a>, 
                         <a href="http://www.cs.sdu.edu.cn/~baoquan/" target="_blank">Baoquan Chen</a> <br>
                         <em>ACM Transactions on Graphics (SIGGRAPH Asia 2014)</em>, 2014 <br>
                         <!--  <strong style="color:green">Best Cognitive Robotics Paper Finalist</strong><br>-->
                         <a href="http://vcc.szu.edu.cn/research/2014/Autoscan/" target="_blank">project</a>
                         /
                         <a href="https://www.dropbox.com/s/3etdpiaybtd54x9/autoscan.pptx?dl=0" target="_blank">slides</a>
                         /
                         <a href="https://www.youtube.com/watch?v=BLX4LbiUtSQ" target="_blank">video (youtube)</a>, 
                         <a href="http://v.youku.com/v_show/id_XNzQ0NTY0Mzk2.html?spm=a2h0k.8191407.0.0&from=s1.8-1-1.2" target="_blank">video (youku)</a>
                         /
                         <a href="https://www.youtube.com/watch?v=hZJNEYyxYtk" target="_blank">live show</a>
                         /
                         <a href="http://vcc.siat.ac.cn/commons/fileupload/upload.do?method=download&uploadId=1532">code</a>
                     </p><p></p>
                     <p>
                         We propose a quality-driven, Poisson-guided autonomous scanning method to ensure the high quality scanning of the model.
                         This goal is achieved by placing the scanner at strategically selected Next-Best-Views (NBVs) to ensure progressively capturing the
                         geometric details of the object, until both completeness and high fidelity are reached.
                     </p>
                 </td>
             </tr>

         </tbody>
     </table>

     <table width="100%" align="center" border="0" cellpadding="20">
         <tbody>
             <tr>
                 <td width="100%" valign="top">
                     <heading>Old Projects</heading>
                     <p>
                         <papertitle>Quadruped Robots</papertitle> <br>
                         Pinxin Long, Hongjin Yu, Haoxing Guo, Ke Zhao, 2010 - 2011
                     </p>
                     <p>
                         We designed several quadruped robots from scratch and implemented discrete reaching movement
                         and rhythmic movements (four different gaits) on these robots using Central Pattern Generator based
                         locomotion control methods. <br>
                     </p>
                     <div style='text-align: center; margin: 0 auto'>
                         <iframe width="440" height="247.5" src="https://www.youtube.com/embed/9zPuFaZ4egA" frameborder="0" allowfullscreen></iframe>
                     </div>
                 </td>

             </tr>
         </tbody>
     </table>

     <table width="100%" align="center" border="0" cellpadding="20">
         <tbody>
             <tr>
                 <td width="100%" valign="top">
                     <p>
                         <papertitle>Speech Controlled Mobile Robot</papertitle> <br>
                         Pinxin Long, Ke Zhao, Jian Zheng, 2010
                     </p>
                     <p>
                         We created a robot using STM32 and LD3320 chips running Chinese Speech Recognition application.
                         This application enables the robot to perform various movement (e.g. move forward, turn left, stop, etc.)
                         based upon user interaction by speech. <br>
                     </p>
                     <div style='text-align: center; margin: 0 auto'>
                         <iframe width="440" height="247.5" src="https://www.youtube.com/embed/-rig5GvlBhI" frameborder="0" allowfullscreen></iframe>
                     </div>
                 </td>
             </tr>
         </tbody>
     </table>

     <!-- 
     <table width="100%" align="center" border="0" cellpadding="20">
         <tbody>
             <tr>
                 <td width="100%" valign="center">
                     <heading>Notes & Thoughts</heading>
                     <p>
                         <heading2>Autonomous Driving</heading2> <br>
                         Route Planning
                         /
                         Behavioral Decision
                         /
                         Motion Planning
                         /
                         Local Feedback Control
                         <br><br>

                         <heading2>Robotics</heading2> <br>
                         Localization
                         / Planning
                         / Control
                         <br><br>

                         <heading2>Reinforcement Learning</heading2> <br>
                         Policy Graident
                         / Driving Policy
                         / Safety RL
                         <br><br>

                         <heading2>Deep Learning</heading2> <br>
                         Optimization
                         / Overfitting
                         <br><br>

                         <heading2>Libraries</heading2> <br>
                         Tensorflow (basics, advanced topics, effective guide)
                         /
                         GPU
                         <br><br>

                     </p>
                 </td>
             </tr>
         </tbody>
     </table>
     -->

     <table width="10%" align="center" border="0" cellspacing="0" cellpadding="0">
          <tbody>
              <tr>
                  <td>
                      <br>
                      <p align="center">
                          <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=4HJn5Wg0Xvl7gBcMbkd1tcgCyeUiI3tMw9caKBmbC84"></script>
                      </p>
                  </td>
              </tr>
          </tbody>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
              <tr>
                  <td>
                      <br>
                      <p align="right"><font size="2">
                          <a href="https://jonbarron.info/" target="_blank">This nice webpage is "stolen" from here.</a>
                      </font>
                      </p>
                  </td>
              </tr>
          </tbody>
      </table>

      <script>
       (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
           (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
       
       ga('create', 'UA-104877365-1', 'auto');
       ga('send', 'pageview');
      </script>
              </td>
          </tr>
      </tbody>
  </table>


</body>
</html>
